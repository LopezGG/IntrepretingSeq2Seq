{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting BERT based Seq2Seq (i.e NER) models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my teens, I found that  cognitive psychology  and biotechnology  could hold the world in suspended animation. There is a great deal of mystery and joy in understanding the delicate and intricate internal processes that lead to external (visible) action. More than a decade later, pytorch based NLP models turned out be a fair enough substitute. So here is an analysis of Named Entity Recognition (NER) Model.<br> The model used here is an NER model fine tuned with BERT-base. It  was obtained from https://github.com/kamalkraj/BERT-NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "<ol>\n",
    "  <li> Take a target word and visualize how every other word contributes to the final tag for the target word </li>\n",
    "  <li> For the target word selected above, zoom into  how much information the comes  word embeddings and position embedding from every word word in the sentence</li>\n",
    "  <li> Zoom one step inside and understand how much each layer of a sample word contributes to the final tag of the target word</li>    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be exploring how the word \"Rob\" in the sentence \"Soccer - Steve Tiger  gets a lucky win , Rob in surprise defeat\" gets tagged as \"B-PER\". We will also look at the effect 12 embedding layers  of the word \"surprise\" have on \"Rob\" being tagged as \"B-PER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Soccer - Steve Tiger  gets a lucky win , Rob in surprise defeat\"\n",
    "target_word ='Rob'\n",
    "token_to_explain_target_Word = 'surprise' # this is used in the last part to understand how each layer \n",
    "#of this word contributed to the tage for the desired word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert import Ner\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerModel = Ner(\"/data/home/gilopez/notebooks/Interpretability/BERT-NER/model/out_base\")\n",
    "# load tokenizer\n",
    "tokenizer = nerModel.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the target word we have to obtain the actual label. As a hack, I am just using label predicted by the model. the code snippet below identifies the tag and word position of the target word. It also identifies the word index of a sample word, the effect of whose  layers will be analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'Rob', 'tag': 'B-PER', 'confidence': 0.9996005892753601}\n",
      "tag B-PER\n",
      "word_position 10\n",
      "tag_index 4\n",
      "{'word': 'surprise', 'tag': 'O', 'confidence': 0.9999698400497437}\n",
      "token_to_explain_index 12\n"
     ]
    }
   ],
   "source": [
    "output = nerModel.predict(text)\n",
    "\n",
    "tag=''\n",
    "word_position = 0 # just initializing. This is word whose tag we are trying to explain\n",
    "rowNum = 0 # we have only 1 sentence we use for prediction\n",
    "\n",
    "for ind,row in enumerate(output):\n",
    "    if row['word'] == target_word:\n",
    "        tag = row['tag']\n",
    "        word_position = ind +1 #for cls token\n",
    "        print(row)\n",
    "        break\n",
    "print(\"tag\",tag)\n",
    "print(\"word_position\",word_position)\n",
    "tag_index = nerModel.label_word_ind[tag]\n",
    "print(\"tag_index\",tag_index)\n",
    "token_to_explain = 0 # the index of the token that we would like to examine each layer of\n",
    "for ind,row in enumerate(output):\n",
    "    if row['word'] == token_to_explain_target_Word:\n",
    "        tag = row['tag']\n",
    "        token_to_explain = ind +1 #for cls token\n",
    "        print(row)\n",
    "        break\n",
    "print(\"token_to_explain_index\",token_to_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to get the tensor values of all the inputs. For this we convert word into indices, and we have a mask to eliminate padded words in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids,input_mask,segment_ids,valid_ids,all_tokens = nerModel.preprocess_infer(text)\n",
    "input_ids = torch.tensor([input_ids],dtype=torch.long,device=nerModel.device)\n",
    "input_mask = torch.tensor([input_mask],dtype=torch.long,device=nerModel.device)\n",
    "segment_ids = torch.tensor([segment_ids],dtype=torch.long,device=nerModel.device)\n",
    "valid_ids = torch.tensor([valid_ids],dtype=torch.long,device=nerModel.device)\n",
    "position_ids = torch.tensor([[x for x in range(len(all_tokens))]],dtype=torch.long,device=nerModel.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 15]),\n",
       " torch.Size([1, 15]),\n",
       " torch.Size([1, 15]),\n",
       " torch.Size([1, 15]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids.shape,input_ids.shape,input_mask.shape,segment_ids.shape # [batch size , num_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While working with attribution based methods we need a reference sentence. In this case, we can substitute  every word in the original sentence with '[PAD]' and use it as reference. The model will first see how the information flows from input to the output from baseline. Then it will determine how the information flow differs for the sentence we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createReferenceSentence(all_tokens,nerModel,tokenizer):\n",
    "    tokens = []\n",
    "    textlen = len(all_tokens) -2  # removing 2 to account to sep & cls\n",
    "    tokens.append(\"[CLS]\")\n",
    "    for i in range(textlen):\n",
    "        tokens.append(tokenizer.pad_token)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    ref_input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    ref_input_ids = torch.tensor([ref_input_ids],dtype=torch.long,device=nerModel.device)\n",
    "    ref_position_ids = torch.tensor([[0 for x in range(len(all_tokens))]],dtype=torch.long,device=nerModel.device)\n",
    "    return ref_input_ids,ref_position_ids,tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference sentence: [CLS] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [SEP]\n"
     ]
    }
   ],
   "source": [
    "ref_input_ids,ref_position_ids,tokens = createReferenceSentence(all_tokens,nerModel,tokenizer)\n",
    "print(\"reference sentence:\" ,' '.join (tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 15]), torch.Size([1, 15]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_position_ids.shape,ref_input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a forward function which will call the model, generate probability distribution of all possible tags. From this we create 2 separate functions\n",
    "1.  One  will  output the value of the most probable tag for the target word. This will be used to find attributions.\n",
    "2. Other will output all the probabilities for all the tags for that target word. This will be used for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_i(input_ids, segment_ids,position_ids, input_mask,valid_ids,word_position=3,rowNum=0):\n",
    "    # rownum is kept at zero because input is a batch of size 1\n",
    "    #we need gradients to calculate intergrated gradients\n",
    "    logits = nerModel.model(input_ids = input_ids, token_type_ids = segment_ids, attention_mask=input_mask,\\\n",
    "                             valid_ids=valid_ids,position_ids=position_ids)\n",
    "    logits = logits[rowNum][word_position].max(0).values.unsqueeze(0) #shape [1]\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_tag_prob(input_ids, segment_ids,position_ids, input_mask,valid_ids,word_position=3,rowNum=0):\n",
    "    word_scores = nerModel.model(input_ids = input_ids, token_type_ids = segment_ids, attention_mask=input_mask,\\\n",
    "                                 valid_ids=valid_ids,position_ids=position_ids)\n",
    "    word_scores = word_scores[rowNum][word_position].unsqueeze(0) # shape must be [1,num_labels]\n",
    "    return word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this layer after predictions with predict_i else it will error out due to hooks\n",
    "lig = LayerIntegratedGradients(predict_i, nerModel.model.bert.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions_word, delta_word = lig.attribute(inputs=input_ids,\n",
    "                                  baselines=ref_input_ids,\n",
    "                                  additional_forward_args=(segment_ids,position_ids,input_mask,valid_ids,word_position),\n",
    "                                  return_convergence_delta=True)\n",
    "attributions_word.shape #[batch_size,num_words, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above call we will get the effect of every embedding dimension of every word on the target word. We will sum it to find the effect of every word in the sentence on the target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks like summing over the embeddings dimension \n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_word_sum = summarize_attributions(attributions_word)# [num_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores = predict_all_tag_prob(input_ids, segment_ids,position_ids, input_mask,valid_ids,word_position,rowNum) # this will be used for final display purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_position_vis = viz.VisualizationDataRecord(\n",
    "                        attributions_word_sum,#word_attributions\n",
    "                        torch.max(torch.softmax(word_scores[0], dim=0)),#pred_prob\n",
    "                        nerModel.label_map[torch.argmax(word_scores).item()],#pred_class\n",
    "                        nerModel.label_map[torch.argmax(word_scores).item()],#true_class\n",
    "                        str(nerModel.label_map[tag_index]),#attr_class\n",
    "                        attributions_word_sum.sum(),  #attr_score     \n",
    "                        all_tokens,#raw_input\n",
    "                        delta_word) #convergence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Visualizations For \" Rob \"\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>B-PER</b></text></td><td><text style=\"padding-right:2em\"><b>B-PER (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>B-PER</b></text></td><td><text style=\"padding-right:2em\"><b>1.11</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Soccer                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> -                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Steve                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Tiger                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> gets                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lucky                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> win                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Rob                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> surprise                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> defeat                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('\\033[1m', 'Visualizations For \"',target_word, '\"\\033[0m')\n",
    "viz.visualize_text([word_position_vis])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image above we see that \"surprise\" contributed most to tagging \"Rob\" as \"B-PER\" while the word \"Rob\" itself contributed negatively to its tagging. To understand this further, we will look into the contribution of the word embedding and position embedding for  the word \"Rob\"  in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of word and position embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a hook on each sublayer of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerModel = Ner(\"/data/home/gilopez/notebooks/Interpretability/BERT-NER/model/out_base\")\n",
    "interpretable_embedding1 = configure_interpretable_embedding_layer(nerModel.model, 'bert.embeddings.word_embeddings')\n",
    "interpretable_embedding2 = configure_interpretable_embedding_layer(nerModel.model, 'bert.embeddings.token_type_embeddings')\n",
    "interpretable_embedding3 = configure_interpretable_embedding_layer(nerModel.model, 'bert.embeddings.position_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next using these hooks , we obtain for input and reference sentence we obtain word embedding, position embedding and token type embedding. Unlike other seq2seq methods, here our token is always 0. We dont use 0 and 1 in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_bert_sub_embedding(input_ids, ref_input_ids,\n",
    "                                   token_type_ids, ref_token_type_ids,\n",
    "                                   position_ids, ref_position_ids):\n",
    "    input_embeddings = interpretable_embedding1.indices_to_embeddings(input_ids)\n",
    "    ref_input_embeddings = interpretable_embedding1.indices_to_embeddings(ref_input_ids)\n",
    "\n",
    "    input_embeddings_token_type = interpretable_embedding2.indices_to_embeddings(token_type_ids)\n",
    "    ref_input_embeddings_token_type = interpretable_embedding2.indices_to_embeddings(ref_token_type_ids)\n",
    "\n",
    "    input_embeddings_position_ids = interpretable_embedding3.indices_to_embeddings(position_ids)\n",
    "    ref_input_embeddings_position_ids = interpretable_embedding3.indices_to_embeddings(ref_position_ids)\n",
    "    \n",
    "    return (input_embeddings, ref_input_embeddings), \\\n",
    "           (input_embeddings_token_type, ref_input_embeddings_token_type), \\\n",
    "           (input_embeddings_position_ids, ref_input_embeddings_position_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function just obtains the words or positions who attributions are within top K. Attributions refer to the contribution of that word towards tagging \"Rob\" as 'B-PER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_attributed_tokens(attrs,all_tokens, k=5):\n",
    "    values, indices = torch.topk(attrs, k)\n",
    "    top_tokens = [all_tokens[idx] for idx in indices]\n",
    "    return top_tokens, values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(input_embed, ref_input_embed), (token_type_ids_embed, ref_token_type_ids_embed), (position_ids_embed, ref_position_ids_embed) = construct_bert_sub_embedding(input_ids, ref_input_ids, \\\n",
    "                                         token_type_ids=segment_ids, ref_token_type_ids=segment_ids, \\\n",
    "                                         position_ids=position_ids, ref_position_ids=ref_position_ids)\n",
    "#input_embed,token_type_ids_embed,ref_token_type_ids_embed : [batch_size,numWords,embed_dim] ie torch.Size([1, 15, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we pass to the captum method IntegratedGradients and get back the attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(predict_i)\n",
    "#input_ids, segment_ids,position_ids, input_mask,valid_ids\n",
    "attributions_word_summary = ig.attribute(inputs=(input_embed, token_type_ids_embed, position_ids_embed),\n",
    "                                  baselines=(ref_input_embed, ref_token_type_ids_embed, ref_position_ids_embed),\n",
    "                                  additional_forward_args=(input_mask,valid_ids,word_position))\n",
    "#(attributions_word_summary[0].shape,attributions_word_summary[1].shape,attributions_word_summary[2].shape) [batch_size,num_word,embed dim ] i.e torch.Size([1, 15, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing it over the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_word = summarize_attributions(attributions_word_summary[0])\n",
    "attributions_word_token_type = summarize_attributions(attributions_word_summary[1])\n",
    "attributions_word_position = summarize_attributions(attributions_word_summary[2])\n",
    "#attributions_word, attributions_word_position #[numWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_interpretable_embedding_layer(nerModel.model, interpretable_embedding1)\n",
    "remove_interpretable_embedding_layer(nerModel.model, interpretable_embedding2)\n",
    "remove_interpretable_embedding_layer(nerModel.model, interpretable_embedding3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next couple of blocks are just formatting and displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=len(all_tokens)\n",
    "top_words, top_words_val, top_word_ind = get_topk_attributed_tokens(attributions_word,all_tokens,k)\n",
    "top_token_type, top_token_type_val, top_token_type_ind = get_topk_attributed_tokens(attributions_word_token_type,all_tokens,k)\n",
    "top_pos, top_pos_val, pos_ind = get_topk_attributed_tokens(attributions_word_position,all_tokens,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word = pd.DataFrame({'Word(Index), Attribution': [\"{} ({}), {}\".format(word, pos, round(val.item(),2)) for word, pos, val in zip(top_words, top_word_ind, top_words_val)],\n",
    "                   'Position(Index), Attribution': [\"{} ({}), {}\".format(position, pos, round(val.item(),2)) for position, pos, val in zip(top_pos, pos_ind, top_pos_val)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['{}({})'.format(token, str(i)) for i, token in enumerate(all_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the word embedding layer for \"Rob\" contributed positively for it being tagged as 'B-PER'. The overall score for contribution of \"Rob\" fell down because of it's word position. <br> Next, we will see how each of the 12 transformer layers of each word in the sentence contributed to tagging \"Rob\" as a 'B-PER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of the 12 transformer Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerModel = Ner(\"/data/home/gilopez/notebooks/Interpretability/BERT-NER/model/out_base\")\n",
    "interpretable_embedding = configure_interpretable_embedding_layer(nerModel.model, 'bert.embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings, ref_input_embeddings = construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                         token_type_ids=segment_ids, ref_token_type_ids=segment_ids, \\\n",
    "                                         position_ids=position_ids, ref_position_ids=ref_position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attrs_word=[]\n",
    "# The token that we would like to examine separately.\n",
    "\n",
    "layer_attrs_token_to_explain_dist = []\n",
    "for i in range(nerModel.model.config.num_hidden_layers):\n",
    "    lc = LayerConductance(predict_i, nerModel.model.bert.encoder.layer[i])\n",
    "    layer_attributions_word = lc.attribute(inputs=input_embeddings, baselines=ref_input_embeddings, additional_forward_args=(segment_ids,position_ids,input_mask,valid_ids,word_position))[0]\n",
    "    # layer_attributions_word : [batch_size,num_words,embedding dimensions]\n",
    "    layer_attrs_word.append(summarize_attributions(layer_attributions_word).cpu().detach().tolist())\n",
    "\n",
    "    # storing attributions of the token id that we would like to examine in more detail in token_to_explain\n",
    "    layer_attrs_token_to_explain_dist.append(layer_attributions_word[0,token_to_explain,:].cpu().detach().tolist())\n",
    "    #[numlayers,embedding_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attributions_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "xticklabels=all_tokens\n",
    "yticklabels=list(range(1,13))\n",
    "ax = sns.heatmap(np.array(layer_attrs_word), xticklabels=xticklabels, yticklabels=yticklabels, linewidth=0.2)\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Layers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the image above, we can see that the first few layers of the words close to \"Rob\" have contributed positivelyto it being tagged as a 'B-PER'. We also find that the last few layers of the target word \"Rob\" also has a strong positive influence. <br>In the above table and the visualizations  , we saw that the word \"surprise\" contributed significantly to \"Rob\" being tagged as 'B-PER'. Lets zoom deeper into  each of the 12 layers for the word \"surprise\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax = sns.boxplot(data=layer_attrs_token_to_explain_dist)\n",
    "plt.xlabel('Layers')\n",
    "plt.ylabel('Attribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the attribution values of each embedding the first 6 layers of the word \"surprise\" is significant. Also the variance of each embedding in the first 6 layers is high compared to the later layers. The attribution values of embeddings in the later layers are tightly clustered towards zero with little variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_interpretable_embedding_layer(nerModel.model, interpretable_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding distribution of attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_attr(attrs, bins=100):\n",
    "    return np.histogram(attrs, bins=bins, density=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attrs_token_to_explain_pdf = map(lambda layer_attrs_token_to_explain_dist: pdf_attr(layer_attrs_token_to_explain_dist), layer_attrs_token_to_explain_dist)\n",
    "layer_attrs_token_to_explain_pdf = np.array(list(layer_attrs_token_to_explain_pdf))\n",
    "\n",
    "# summing attribution along embedding diemension for each layer\n",
    "# size: #layers\n",
    "attr_sum = np.array(layer_attrs_token_to_explain_dist).sum(-1)\n",
    "\n",
    "# size: #layers\n",
    "layer_attrs_token_to_explain_pdf_norm = np.linalg.norm(layer_attrs_token_to_explain_pdf, axis=-1, ord=1)\n",
    "\n",
    "#size: #bins x #layers\n",
    "layer_attrs_token_to_explain_pdf = np.transpose(layer_attrs_token_to_explain_pdf)\n",
    "\n",
    "#size: #bins x #layers\n",
    "layer_attrs_token_to_explain_pdf = np.divide(layer_attrs_token_to_explain_pdf, layer_attrs_token_to_explain_pdf_norm, where=layer_attrs_token_to_explain_pdf_norm!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(layer_attrs_token_to_explain_pdf)\n",
    "plt.xlabel('Bins')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(['Layer '+ str(i) for i in range(1,13)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribution values of  most of the layers follows a uniform distribution. Layer 2 does have a different curve and needs to be analyzed further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://captum.ai/tutorials/Bert_SQUAD_Interpret\n",
    "2. https://github.com/MichaMucha/awdlstm-integrated-gradients/blob/master/explaining_predictions_awdlstm.ipynb\n",
    "3. https://towardsdatascience.com/open-the-black-box-understand-what-drives-predictions-in-deep-nlp-models-833f3dc923d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpret_k1",
   "language": "python",
   "name": "interpret_k1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
